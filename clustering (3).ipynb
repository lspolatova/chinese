{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084f5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from data_lib import Data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6e6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_for_clustering = Data('chinese-word-list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b277f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8175/8175 [00:13<00:00, 593.02it/s]\n"
     ]
    }
   ],
   "source": [
    "Data_for_clustering.drawings(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232946b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "驭\n"
     ]
    }
   ],
   "source": [
    "num = np.random.randint(0, len(Data_for_clustering.X_data))\n",
    "fig = plt.figure()\n",
    "plt.imshow(Data_for_clustering.X_data[num])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(Data_for_clustering.Y_data[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c38b120",
   "metadata": {},
   "outputs": [],
   "source": [
    " Data_for_clustering.lable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d78d7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8175, 8175)\n",
      "(8175, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Data_for_clustering.Y_classes))\n",
    "print(np.shape(Data_for_clustering.X_data)) #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f7f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = load_model('resnet50_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e779ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 15, 15, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 15, 15, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 15, 15, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 15, 15, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 15, 15, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 15, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 15, 15, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 15, 15, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 8, 8, 128)    32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 128)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 8, 8, 512)    131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 8, 8, 512)    2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 512)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 512)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 4, 4, 256)    131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 4, 4, 1024)   525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 4, 4, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 1024)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 2, 2, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 2, 2, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 2, 2, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 2, 2, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 2, 2, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 2, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 2, 2, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 2, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 2, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 2, 2, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 2, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc8175 (Dense)                  (None, 8175)         16750575    flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,338,287\n",
      "Trainable params: 40,285,167\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d8a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 42s 165ms/step - loss: 0.1585 - accuracy: 0.9601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15849635004997253, 0.9601223468780518]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet.evaluate(Data_for_clustering.X_data, Data_for_clustering.Y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb0ccca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['抔', '访', '洗', '赵', '吡'], dtype='<U1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_for_clustering.hieroglyphs(model_resnet.predict(Data_for_clustering.X_data)[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b2406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['抔', '访', '洗', '赵', '吡']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_for_clustering.Y_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f23b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 15, 15, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 15, 15, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 15, 15, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 15, 15, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 15, 15, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 15, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 15, 15, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 15, 15, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 15, 15, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 15, 15, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 15, 15, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 15, 15, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 15, 15, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 8, 8, 128)    32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 128)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 8, 8, 512)    131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 8, 8, 512)    2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 512)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 512)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 8, 8, 128)    65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 8, 8, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 8, 8, 512)    66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 8, 8, 512)    2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 4, 4, 256)    131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 4, 4, 1024)   525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 4, 4, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 1024)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 4, 4, 256)    262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 4, 4, 256)    590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 4, 4, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 4, 4, 1024)   263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 4, 4, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 2, 2, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 2, 2, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 2, 2, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 2, 2, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 2, 2, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 2, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 2, 2, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 2, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 2, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 2, 2, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 2, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "model_resnet= Model(inputs=model_resnet.input, outputs=model_resnet.layers[-2].output)\n",
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38175f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=30).fit_predict(preprocessing.normalize(model_resnet(Data_for_clustering.X_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e73208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['岱',\n",
       " '帚',\n",
       " '茚',\n",
       " '语',\n",
       " '岢',\n",
       " '卺',\n",
       " '篙',\n",
       " '督',\n",
       " '嚅',\n",
       " '责',\n",
       " '带',\n",
       " '苦',\n",
       " '芾',\n",
       " '借',\n",
       " '哲',\n",
       " '葫',\n",
       " '眘',\n",
       " '僔',\n",
       " '窗',\n",
       " '鲁',\n",
       " '荀',\n",
       " '苪',\n",
       " '詟',\n",
       " '自',\n",
       " '誉',\n",
       " '甘',\n",
       " '晦',\n",
       " '笃',\n",
       " '嵩',\n",
       " '悟',\n",
       " '誊',\n",
       " '筲',\n",
       " '匍',\n",
       " '含',\n",
       " '哥',\n",
       " '倚',\n",
       " '昔',\n",
       " '黹',\n",
       " '蔺',\n",
       " '甫',\n",
       " '需',\n",
       " '简',\n",
       " '管',\n",
       " '萜',\n",
       " '僻',\n",
       " '肯',\n",
       " '胄',\n",
       " '黄',\n",
       " '卤',\n",
       " '蒔',\n",
       " '筒',\n",
       " '菁',\n",
       " '岺',\n",
       " '晋',\n",
       " '昺',\n",
       " '萄',\n",
       " '肴',\n",
       " '苍',\n",
       " '甾',\n",
       " '畜',\n",
       " '窝',\n",
       " '曹',\n",
       " '簧',\n",
       " '岧',\n",
       " '喾',\n",
       " '俘',\n",
       " '偌',\n",
       " '雋',\n",
       " '背',\n",
       " '帝',\n",
       " '告',\n",
       " '雪',\n",
       " '莴',\n",
       " '郜',\n",
       " '甭',\n",
       " '白',\n",
       " '苗',\n",
       " '胬',\n",
       " '膏',\n",
       " '官',\n",
       " '芑',\n",
       " '倭',\n",
       " '昝',\n",
       " '雷',\n",
       " '箸',\n",
       " '宦',\n",
       " '蒂',\n",
       " '亩',\n",
       " '崙',\n",
       " '骨',\n",
       " '茜',\n",
       " '凿',\n",
       " '菩',\n",
       " '节',\n",
       " '岳',\n",
       " '峇',\n",
       " '倦',\n",
       " '訇',\n",
       " '富',\n",
       " '俦',\n",
       " '俤',\n",
       " '筍',\n",
       " '萤',\n",
       " '书',\n",
       " '莒',\n",
       " '蓍',\n",
       " '音',\n",
       " '啬',\n",
       " '着',\n",
       " '尊',\n",
       " '窜',\n",
       " '佾',\n",
       " '鼋',\n",
       " '宫',\n",
       " '贫',\n",
       " '肓',\n",
       " '善',\n",
       " '茵',\n",
       " '沓',\n",
       " '蛋',\n",
       " '崮',\n",
       " '奇',\n",
       " '旮',\n",
       " '茴',\n",
       " '暂',\n",
       " '皙',\n",
       " '伶',\n",
       " '啭',\n",
       " '者',\n",
       " '奁',\n",
       " '俉',\n",
       " '镥',\n",
       " '畄',\n",
       " '笤',\n",
       " '苘',\n",
       " '垂',\n",
       " '酋',\n",
       " '首',\n",
       " '誓',\n",
       " '昚',\n",
       " '葡',\n",
       " '皆',\n",
       " '害',\n",
       " '巷',\n",
       " '警',\n",
       " '卷',\n",
       " '芭',\n",
       " '笞',\n",
       " '臼',\n",
       " '茸',\n",
       " '闺',\n",
       " '赀',\n",
       " '帑',\n",
       " '宥',\n",
       " '百',\n",
       " '芮',\n",
       " '蔷',\n",
       " '杏',\n",
       " '马',\n",
       " '算',\n",
       " '青',\n",
       " '鹊',\n",
       " '赏',\n",
       " '肾',\n",
       " '吉',\n",
       " '臂',\n",
       " '箇',\n",
       " '鼍',\n",
       " '胥',\n",
       " '旨',\n",
       " '蕾',\n",
       " '畚',\n",
       " '黉',\n",
       " '催',\n",
       " '蒿',\n",
       " '击',\n",
       " '省',\n",
       " '哿',\n",
       " '凭',\n",
       " '訚',\n",
       " '专',\n",
       " '兽',\n",
       " '香',\n",
       " '曾',\n",
       " '篱',\n",
       " '卣',\n",
       " '菌',\n",
       " '奤',\n",
       " '眚',\n",
       " '司',\n",
       " '晳',\n",
       " '看',\n",
       " '冑',\n",
       " '苟',\n",
       " '番',\n",
       " '罟',\n",
       " '吾',\n",
       " '宵',\n",
       " '窨',\n",
       " '笍',\n",
       " '嵌',\n",
       " '苷',\n",
       " '苫',\n",
       " '罾',\n",
       " '霅',\n",
       " '箐',\n",
       " '昏',\n",
       " '莆',\n",
       " '嶲',\n",
       " '筶',\n",
       " '罔',\n",
       " '苔',\n",
       " '菖',\n",
       " '裔',\n",
       " '古',\n",
       " '膂',\n",
       " '苌',\n",
       " '畬',\n",
       " '宙',\n",
       " '爸',\n",
       " '蒨',\n",
       " '隋',\n",
       " '尚',\n",
       " '笹',\n",
       " '与',\n",
       " '芒',\n",
       " '蓓',\n",
       " '菑',\n",
       " '弩',\n",
       " '贪',\n",
       " '荒',\n",
       " '替',\n",
       " '鸢',\n",
       " '苜',\n",
       " '啻',\n",
       " '嵛',\n",
       " '茕',\n",
       " '冒',\n",
       " '普',\n",
       " '常',\n",
       " '怿',\n",
       " '彗',\n",
       " '菅',\n",
       " '倍',\n",
       " '合',\n",
       " '肖',\n",
       " '耆',\n",
       " '酱',\n",
       " '再',\n",
       " '营',\n",
       " '苛',\n",
       " '售',\n",
       " '杳',\n",
       " '茼',\n",
       " '舌',\n",
       " '誾',\n",
       " '葺',\n",
       " '傍',\n",
       " '矞',\n",
       " '赁',\n",
       " '傅',\n",
       " '萑',\n",
       " '鵟',\n",
       " '笥',\n",
       " '盲',\n",
       " '冐',\n",
       " '蓄',\n",
       " '斋',\n",
       " '箭',\n",
       " '育',\n",
       " '舍',\n",
       " '耑',\n",
       " '甯',\n",
       " '赍',\n",
       " '窅',\n",
       " '仔',\n",
       " '芎',\n",
       " '詈',\n",
       " '霄',\n",
       " '嵆',\n",
       " '苇',\n",
       " '龠',\n",
       " '言',\n",
       " '篑',\n",
       " '萏',\n",
       " '齿',\n",
       " '笛',\n",
       " '舀']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Data_for_clustering.Y_data[i] for i in [index for (index, item) in enumerate(kmeans) if item == 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d70b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 645825/645825 [21:17<00:00, 505.69it/s]\n"
     ]
    }
   ],
   "source": [
    " Data_for_clustering.drawings(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8529f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasters_kmeans = tf.keras.utils.to_categorical(list(kmeans) * 80,  dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62747dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654000, 30)\n",
      "(654000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(clasters_kmeans))\n",
    "print(np.shape(Data_for_clustering.X_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36299278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bb66732",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNet = tf.keras.applications.MobileNet(input_shape=(64, 64, 3), weights=None, classes=30, classifier_activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e654e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNet.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c3d7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение первого уровня\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "filepath_to_raw_model=\"content/MobileNet_current_30.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath_to_raw_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "callbacks_lists = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb1c8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18394/18394 [==============================] - ETA: 0s - loss: 1.4471 - accuracy: 0.5419\n",
      "Epoch 00001: val_loss improved from inf to 0.81601, saving model to content\\MobileNet_current_30.hdf5\n",
      "18394/18394 [==============================] - 12067s 656ms/step - loss: 1.4471 - accuracy: 0.5419 - val_loss: 0.8160 - val_accuracy: 0.7217\n",
      "Epoch 2/20\n",
      "18394/18394 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.7741\n",
      "Epoch 00002: val_loss improved from 0.81601 to 0.39064, saving model to content\\MobileNet_current_30.hdf5\n",
      "18394/18394 [==============================] - 24205s 1s/step - loss: 0.6562 - accuracy: 0.7741 - val_loss: 0.3906 - val_accuracy: 0.8665\n",
      "Epoch 3/20\n",
      "18394/18394 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8721\n",
      "Epoch 00003: val_loss improved from 0.39064 to 0.28693, saving model to content\\MobileNet_current_30.hdf5\n",
      "18394/18394 [==============================] - 9805s 533ms/step - loss: 0.3695 - accuracy: 0.8721 - val_loss: 0.2869 - val_accuracy: 0.9018\n",
      "Epoch 4/20\n",
      "18394/18394 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9272\n",
      "Epoch 00004: val_loss improved from 0.28693 to 0.11274, saving model to content\\MobileNet_current_30.hdf5\n",
      "18394/18394 [==============================] - 6772s 368ms/step - loss: 0.2137 - accuracy: 0.9272 - val_loss: 0.1127 - val_accuracy: 0.9638\n",
      "Epoch 5/20\n",
      "  245/18394 [..............................] - ETA: 1:57:19 - loss: 0.1556 - accuracy: 0.9495"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-b978e5bcbe4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMobileNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData_for_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasters_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_lists\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MobileNet.fit(Data_for_clustering.X_data, clasters_kmeans, epochs = 20, batch_size = 32,validation_split = 0.1, callbacks=callbacks_lists,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d938561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "def LeNet(nb_classes, input_shape = (64, 64, 3)):\n",
    "    X_input = layers.Input(input_shape)\n",
    "    X = layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"same\")(X_input)\n",
    "    X = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(X)\n",
    "    X = layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid')(X)\n",
    "    X = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(X)\n",
    "    X = layers.Flatten()(X)\n",
    "    X = layers.Dense(120, activation='tanh')(X)\n",
    "    X = layers.Dense(84, activation='tanh')(X)\n",
    "    X = layers.Dense(nb_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='LeNet')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "336f0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenet0 22800\n",
      "Epoch 1/5\n",
      "641/642 [============================>.] - ETA: 0s - loss: 5.0489 - accuracy: 0.1672\n",
      "Epoch 00001: val_loss improved from inf to 4.04102, saving model to Lenet0.hdf5\n",
      "642/642 [==============================] - 53s 83ms/step - loss: 5.0487 - accuracy: 0.1673 - val_loss: 4.0410 - val_accuracy: 0.3724\n",
      "Epoch 2/5\n",
      "641/642 [============================>.] - ETA: 0s - loss: 2.6596 - accuracy: 0.6258\n",
      "Epoch 00002: val_loss improved from 4.04102 to 1.83955, saving model to Lenet0.hdf5\n",
      "642/642 [==============================] - 53s 82ms/step - loss: 2.6590 - accuracy: 0.6259 - val_loss: 1.8396 - val_accuracy: 0.7417\n",
      "Epoch 3/5\n",
      "641/642 [============================>.] - ETA: 0s - loss: 0.9545 - accuracy: 0.8930\n",
      "Epoch 00003: val_loss improved from 1.83955 to 0.78702, saving model to Lenet0.hdf5\n",
      "642/642 [==============================] - 53s 82ms/step - loss: 0.9546 - accuracy: 0.8930 - val_loss: 0.7870 - val_accuracy: 0.9053\n",
      "Epoch 4/5\n",
      "641/642 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.9712\n",
      "Epoch 00004: val_loss improved from 0.78702 to 0.36839, saving model to Lenet0.hdf5\n",
      "642/642 [==============================] - 52s 82ms/step - loss: 0.3466 - accuracy: 0.9712 - val_loss: 0.3684 - val_accuracy: 0.9658\n",
      "Epoch 5/5\n",
      "641/642 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9931\n",
      "Epoch 00005: val_loss improved from 0.36839 to 0.21274, saving model to Lenet0.hdf5\n",
      "642/642 [==============================] - 53s 83ms/step - loss: 0.1380 - accuracy: 0.9931 - val_loss: 0.2127 - val_accuracy: 0.9798\n",
      "Lenet1 19120\n",
      "Epoch 1/5\n",
      "538/538 [==============================] - ETA: 0s - loss: 4.6539 - accuracy: 0.1727\n",
      "Epoch 00001: val_loss improved from inf to 3.36351, saving model to Lenet1.hdf5\n",
      "538/538 [==============================] - 45s 84ms/step - loss: 4.6539 - accuracy: 0.1727 - val_loss: 3.3635 - val_accuracy: 0.4236\n",
      "Epoch 2/5\n",
      "538/538 [==============================] - ETA: 0s - loss: 2.1991 - accuracy: 0.6710\n",
      "Epoch 00002: val_loss improved from 3.36351 to 1.51667, saving model to Lenet1.hdf5\n",
      "538/538 [==============================] - 45s 84ms/step - loss: 2.1991 - accuracy: 0.6710 - val_loss: 1.5167 - val_accuracy: 0.7829\n",
      "Epoch 3/5\n",
      "538/538 [==============================] - ETA: 0s - loss: 0.8565 - accuracy: 0.9056\n",
      "Epoch 00003: val_loss improved from 1.51667 to 0.67295, saving model to Lenet1.hdf5\n",
      "538/538 [==============================] - 44s 83ms/step - loss: 0.8565 - accuracy: 0.9056 - val_loss: 0.6730 - val_accuracy: 0.9205\n",
      "Epoch 4/5\n",
      "538/538 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.9751\n",
      "Epoch 00004: val_loss improved from 0.67295 to 0.32969, saving model to Lenet1.hdf5\n",
      "538/538 [==============================] - 45s 84ms/step - loss: 0.3364 - accuracy: 0.9751 - val_loss: 0.3297 - val_accuracy: 0.9697\n",
      "Epoch 5/5\n",
      "538/538 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9941\n",
      "Epoch 00005: val_loss improved from 0.32969 to 0.19109, saving model to Lenet1.hdf5\n",
      "538/538 [==============================] - 45s 84ms/step - loss: 0.1450 - accuracy: 0.9941 - val_loss: 0.1911 - val_accuracy: 0.9812\n",
      "Lenet2 20080\n",
      "Epoch 1/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 4.8242 - accuracy: 0.1834\n",
      "Epoch 00001: val_loss improved from inf to 3.56992, saving model to Lenet2.hdf5\n",
      "565/565 [==============================] - 46s 82ms/step - loss: 4.8242 - accuracy: 0.1834 - val_loss: 3.5699 - val_accuracy: 0.4348\n",
      "Epoch 2/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 2.1971 - accuracy: 0.7017\n",
      "Epoch 00002: val_loss improved from 3.56992 to 1.41027, saving model to Lenet2.hdf5\n",
      "565/565 [==============================] - 46s 81ms/step - loss: 2.1971 - accuracy: 0.7017 - val_loss: 1.4103 - val_accuracy: 0.8182\n",
      "Epoch 3/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.9271\n",
      "Epoch 00003: val_loss improved from 1.41027 to 0.56752, saving model to Lenet2.hdf5\n",
      "565/565 [==============================] - 45s 80ms/step - loss: 0.7297 - accuracy: 0.9271 - val_loss: 0.5675 - val_accuracy: 0.9368\n",
      "Epoch 4/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9833\n",
      "Epoch 00004: val_loss improved from 0.56752 to 0.28285, saving model to Lenet2.hdf5\n",
      "565/565 [==============================] - 44s 79ms/step - loss: 0.2623 - accuracy: 0.9833 - val_loss: 0.2828 - val_accuracy: 0.9711\n",
      "Epoch 5/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9963\n",
      "Epoch 00005: val_loss improved from 0.28285 to 0.17576, saving model to Lenet2.hdf5\n",
      "565/565 [==============================] - 40s 71ms/step - loss: 0.1069 - accuracy: 0.9963 - val_loss: 0.1758 - val_accuracy: 0.9811\n",
      "Lenet3 19200\n",
      "Epoch 1/5\n",
      "540/540 [==============================] - ETA: 0s - loss: 4.7348 - accuracy: 0.1784\n",
      "Epoch 00001: val_loss improved from inf to 3.48834, saving model to Lenet3.hdf5\n",
      "540/540 [==============================] - 43s 79ms/step - loss: 4.7348 - accuracy: 0.1784 - val_loss: 3.4883 - val_accuracy: 0.4214\n",
      "Epoch 2/5\n",
      "540/540 [==============================] - ETA: 0s - loss: 2.2772 - accuracy: 0.6711\n",
      "Epoch 00002: val_loss improved from 3.48834 to 1.47597, saving model to Lenet3.hdf5\n",
      "540/540 [==============================] - 47s 86ms/step - loss: 2.2772 - accuracy: 0.6711 - val_loss: 1.4760 - val_accuracy: 0.8042\n",
      "Epoch 3/5\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8174 - accuracy: 0.9150\n",
      "Epoch 00003: val_loss improved from 1.47597 to 0.63600, saving model to Lenet3.hdf5\n",
      "540/540 [==============================] - 44s 82ms/step - loss: 0.8174 - accuracy: 0.9150 - val_loss: 0.6360 - val_accuracy: 0.9318\n",
      "Epoch 4/5\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.9786\n",
      "Epoch 00004: val_loss improved from 0.63600 to 0.30275, saving model to Lenet3.hdf5\n",
      "540/540 [==============================] - 42s 77ms/step - loss: 0.3045 - accuracy: 0.9786 - val_loss: 0.3028 - val_accuracy: 0.9688\n",
      "Epoch 5/5\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9946\n",
      "Epoch 00005: val_loss improved from 0.30275 to 0.16348, saving model to Lenet3.hdf5\n",
      "540/540 [==============================] - 42s 78ms/step - loss: 0.1294 - accuracy: 0.9946 - val_loss: 0.1635 - val_accuracy: 0.9854\n",
      "Lenet4 21760\n",
      "Epoch 1/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 4.8208 - accuracy: 0.1959\n",
      "Epoch 00001: val_loss improved from inf to 3.65534, saving model to Lenet4.hdf5\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 4.8208 - accuracy: 0.1959 - val_loss: 3.6553 - val_accuracy: 0.4205\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 2.2653 - accuracy: 0.6841\n",
      "Epoch 00002: val_loss improved from 3.65534 to 1.51296, saving model to Lenet4.hdf5\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 2.2653 - accuracy: 0.6841 - val_loss: 1.5130 - val_accuracy: 0.7946\n",
      "Epoch 3/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.7656 - accuracy: 0.9251\n",
      "Epoch 00003: val_loss improved from 1.51296 to 0.66296, saving model to Lenet4.hdf5\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.7656 - accuracy: 0.9251 - val_loss: 0.6630 - val_accuracy: 0.9136\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9818\n",
      "Epoch 00004: val_loss improved from 0.66296 to 0.34444, saving model to Lenet4.hdf5\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.2750 - accuracy: 0.9818 - val_loss: 0.3444 - val_accuracy: 0.9600\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9955\n",
      "Epoch 00005: val_loss improved from 0.34444 to 0.22129, saving model to Lenet4.hdf5\n",
      "612/612 [==============================] - 51s 83ms/step - loss: 0.1134 - accuracy: 0.9955 - val_loss: 0.2213 - val_accuracy: 0.9715\n",
      "Lenet5 23200\n",
      "Epoch 1/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 5.7309 - accuracy: 0.0032\n",
      "Epoch 00001: val_loss improved from inf to 5.71610, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 53s 81ms/step - loss: 5.7309 - accuracy: 0.0032 - val_loss: 5.7161 - val_accuracy: 0.0034\n",
      "Epoch 2/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 5.7160 - accuracy: 0.0035\n",
      "Epoch 00002: val_loss did not improve from 5.71610\n",
      "653/653 [==============================] - 52s 80ms/step - loss: 5.7159 - accuracy: 0.0034 - val_loss: 5.7166 - val_accuracy: 0.0034\n",
      "Epoch 3/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 5.7070 - accuracy: 0.0037\n",
      "Epoch 00003: val_loss improved from 5.71610 to 5.69773, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 53s 81ms/step - loss: 5.7070 - accuracy: 0.0037 - val_loss: 5.6977 - val_accuracy: 0.0043\n",
      "Epoch 4/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 5.6936 - accuracy: 0.0033\n",
      "Epoch 00004: val_loss improved from 5.69773 to 5.67499, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 53s 82ms/step - loss: 5.6937 - accuracy: 0.0033 - val_loss: 5.6750 - val_accuracy: 0.0047\n",
      "Epoch 5/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 5.6581 - accuracy: 0.0057\n",
      "Epoch 00005: val_loss improved from 5.67499 to 5.62251, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 53s 82ms/step - loss: 5.6581 - accuracy: 0.0057 - val_loss: 5.6225 - val_accuracy: 0.0060\n",
      "Lenet5 23200\n",
      "Epoch 1/5\n",
      "653/653 [==============================] - ETA: 0s - loss: 4.8203 - accuracy: 0.1703\n",
      "Epoch 00001: val_loss improved from 5.62251 to 3.42321, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 54s 83ms/step - loss: 4.8203 - accuracy: 0.1703 - val_loss: 3.4232 - val_accuracy: 0.3974\n",
      "Epoch 2/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.1801 - accuracy: 0.6798\n",
      "Epoch 00002: val_loss improved from 3.42321 to 1.51896, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 59s 90ms/step - loss: 2.1795 - accuracy: 0.6799 - val_loss: 1.5190 - val_accuracy: 0.7750\n",
      "Epoch 3/5\n",
      "653/653 [==============================] - ETA: 0s - loss: 0.8145 - accuracy: 0.9123\n",
      "Epoch 00003: val_loss improved from 1.51896 to 0.68138, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 56s 86ms/step - loss: 0.8145 - accuracy: 0.9123 - val_loss: 0.6814 - val_accuracy: 0.9112\n",
      "Epoch 4/5\n",
      "652/653 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.9749\n",
      "Epoch 00004: val_loss improved from 0.68138 to 0.35773, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 57s 88ms/step - loss: 0.3254 - accuracy: 0.9749 - val_loss: 0.3577 - val_accuracy: 0.9599\n",
      "Epoch 5/5\n",
      "653/653 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9940\n",
      "Epoch 00005: val_loss improved from 0.35773 to 0.20456, saving model to Lenet5.hdf5\n",
      "653/653 [==============================] - 57s 87ms/step - loss: 0.1420 - accuracy: 0.9940 - val_loss: 0.2046 - val_accuracy: 0.9733\n",
      "Lenet6 21920\n",
      "Epoch 1/5\n",
      "617/617 [==============================] - ETA: 0s - loss: 4.4188 - accuracy: 0.1834\n",
      "Epoch 00001: val_loss improved from inf to 2.95048, saving model to Lenet6.hdf5\n",
      "617/617 [==============================] - 53s 86ms/step - loss: 4.4188 - accuracy: 0.1834 - val_loss: 2.9505 - val_accuracy: 0.4740\n",
      "Epoch 2/5\n",
      "617/617 [==============================] - ETA: 0s - loss: 1.9393 - accuracy: 0.6880\n",
      "Epoch 00002: val_loss improved from 2.95048 to 1.30140, saving model to Lenet6.hdf5\n",
      "617/617 [==============================] - 53s 85ms/step - loss: 1.9393 - accuracy: 0.6880 - val_loss: 1.3014 - val_accuracy: 0.8089\n",
      "Epoch 3/5\n",
      "617/617 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.9087\n",
      "Epoch 00003: val_loss improved from 1.30140 to 0.60544, saving model to Lenet6.hdf5\n",
      "617/617 [==============================] - 52s 84ms/step - loss: 0.7934 - accuracy: 0.9087 - val_loss: 0.6054 - val_accuracy: 0.9293\n",
      "Epoch 4/5\n",
      "616/617 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9763\n",
      "Epoch 00004: val_loss improved from 0.60544 to 0.30944, saving model to Lenet6.hdf5\n",
      "617/617 [==============================] - 52s 85ms/step - loss: 0.3330 - accuracy: 0.9763 - val_loss: 0.3094 - val_accuracy: 0.9772\n",
      "Epoch 5/5\n",
      "617/617 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9943\n",
      "Epoch 00005: val_loss improved from 0.30944 to 0.16803, saving model to Lenet6.hdf5\n",
      "617/617 [==============================] - 53s 86ms/step - loss: 0.1514 - accuracy: 0.9943 - val_loss: 0.1680 - val_accuracy: 0.9849\n",
      "Lenet7 18880\n",
      "Epoch 1/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 5.5299 - accuracy: 0.0042\n",
      "Epoch 00001: val_loss improved from inf to 5.50959, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 44s 83ms/step - loss: 5.5298 - accuracy: 0.0042 - val_loss: 5.5096 - val_accuracy: 0.0042\n",
      "Epoch 2/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 5.5154 - accuracy: 0.0054\n",
      "Epoch 00002: val_loss improved from 5.50959 to 5.50863, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 5.5155 - accuracy: 0.0054 - val_loss: 5.5086 - val_accuracy: 0.0042\n",
      "Epoch 3/5\n",
      "531/531 [==============================] - ETA: 0s - loss: 5.5101 - accuracy: 0.0031\n",
      "Epoch 00003: val_loss improved from 5.50863 to 5.50708, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 5.5101 - accuracy: 0.0031 - val_loss: 5.5071 - val_accuracy: 0.0037\n",
      "Epoch 4/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 5.5077 - accuracy: 0.0042\n",
      "Epoch 00004: val_loss improved from 5.50708 to 5.49539, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 5.5078 - accuracy: 0.0042 - val_loss: 5.4954 - val_accuracy: 0.0042\n",
      "Epoch 5/5\n",
      "531/531 [==============================] - ETA: 0s - loss: 5.5022 - accuracy: 0.0044\n",
      "Epoch 00005: val_loss did not improve from 5.49539\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 5.5022 - accuracy: 0.0044 - val_loss: 5.4956 - val_accuracy: 0.0042\n",
      "Lenet7 18880\n",
      "Epoch 1/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 4.8104 - accuracy: 0.1567\n",
      "Epoch 00001: val_loss improved from 5.49539 to 3.66503, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 4.8091 - accuracy: 0.1568 - val_loss: 3.6650 - val_accuracy: 0.3861\n",
      "Epoch 2/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 2.4046 - accuracy: 0.6475\n",
      "Epoch 00002: val_loss improved from 3.66503 to 1.64893, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 2.4032 - accuracy: 0.6478 - val_loss: 1.6489 - val_accuracy: 0.7707\n",
      "Epoch 3/5\n",
      "531/531 [==============================] - ETA: 0s - loss: 0.8844 - accuracy: 0.9050\n",
      "Epoch 00003: val_loss improved from 1.64893 to 0.71996, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.8844 - accuracy: 0.9050 - val_loss: 0.7200 - val_accuracy: 0.9153\n",
      "Epoch 4/5\n",
      "531/531 [==============================] - ETA: 0s - loss: 0.3293 - accuracy: 0.9759\n",
      "Epoch 00004: val_loss improved from 0.71996 to 0.35927, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.3293 - accuracy: 0.9759 - val_loss: 0.3593 - val_accuracy: 0.9555\n",
      "Epoch 5/5\n",
      "530/531 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9936\n",
      "Epoch 00005: val_loss improved from 0.35927 to 0.21351, saving model to Lenet7.hdf5\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.1389 - accuracy: 0.9936 - val_loss: 0.2135 - val_accuracy: 0.9709\n",
      "Lenet8 21760\n",
      "Epoch 1/5\n",
      "611/612 [============================>.] - ETA: 0s - loss: 4.8043 - accuracy: 0.1927\n",
      "Epoch 00001: val_loss improved from inf to 3.53864, saving model to Lenet8.hdf5\n",
      "612/612 [==============================] - 25s 41ms/step - loss: 4.8027 - accuracy: 0.1930 - val_loss: 3.5386 - val_accuracy: 0.4251\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 2.1570 - accuracy: 0.6970\n",
      "Epoch 00002: val_loss improved from 3.53864 to 1.43154, saving model to Lenet8.hdf5\n",
      "612/612 [==============================] - 25s 41ms/step - loss: 2.1570 - accuracy: 0.6970 - val_loss: 1.4315 - val_accuracy: 0.8056\n",
      "Epoch 3/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.7458 - accuracy: 0.9204\n",
      "Epoch 00003: val_loss improved from 1.43154 to 0.62697, saving model to Lenet8.hdf5\n",
      "612/612 [==============================] - 25s 41ms/step - loss: 0.7458 - accuracy: 0.9204 - val_loss: 0.6270 - val_accuracy: 0.9265\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9797\n",
      "Epoch 00004: val_loss improved from 0.62697 to 0.30108, saving model to Lenet8.hdf5\n",
      "612/612 [==============================] - 24s 40ms/step - loss: 0.2800 - accuracy: 0.9797 - val_loss: 0.3011 - val_accuracy: 0.9701\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9953\n",
      "Epoch 00005: val_loss improved from 0.30108 to 0.17914, saving model to Lenet8.hdf5\n",
      "612/612 [==============================] - 25s 40ms/step - loss: 0.1187 - accuracy: 0.9953 - val_loss: 0.1791 - val_accuracy: 0.9816\n",
      "Lenet9 22720\n",
      "Epoch 1/5\n",
      "639/639 [==============================] - ETA: 0s - loss: 4.8324 - accuracy: 0.1931\n",
      "Epoch 00001: val_loss improved from inf to 3.59519, saving model to Lenet9.hdf5\n",
      "639/639 [==============================] - 26s 40ms/step - loss: 4.8324 - accuracy: 0.1931 - val_loss: 3.5952 - val_accuracy: 0.4269\n",
      "Epoch 2/5\n",
      "638/639 [============================>.] - ETA: 0s - loss: 2.2453 - accuracy: 0.6903\n",
      "Epoch 00002: val_loss improved from 3.59519 to 1.50193, saving model to Lenet9.hdf5\n",
      "639/639 [==============================] - 26s 40ms/step - loss: 2.2440 - accuracy: 0.6903 - val_loss: 1.5019 - val_accuracy: 0.8041\n",
      "Epoch 3/5\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.7604 - accuracy: 0.9207\n",
      "Epoch 00003: val_loss improved from 1.50193 to 0.62072, saving model to Lenet9.hdf5\n",
      "639/639 [==============================] - 26s 40ms/step - loss: 0.7597 - accuracy: 0.9208 - val_loss: 0.6207 - val_accuracy: 0.9327\n",
      "Epoch 4/5\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9814\n",
      "Epoch 00004: val_loss improved from 0.62072 to 0.32114, saving model to Lenet9.hdf5\n",
      "639/639 [==============================] - 26s 40ms/step - loss: 0.2712 - accuracy: 0.9814 - val_loss: 0.3211 - val_accuracy: 0.9652\n",
      "Epoch 5/5\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9955\n",
      "Epoch 00005: val_loss improved from 0.32114 to 0.19799, saving model to Lenet9.hdf5\n",
      "639/639 [==============================] - 26s 41ms/step - loss: 0.1098 - accuracy: 0.9955 - val_loss: 0.1980 - val_accuracy: 0.9767\n",
      "Lenet10 23600\n",
      "Epoch 1/5\n",
      "664/664 [==============================] - ETA: 0s - loss: 4.8451 - accuracy: 0.1994\n",
      "Epoch 00001: val_loss improved from inf to 3.41855, saving model to Lenet10.hdf5\n",
      "664/664 [==============================] - 52s 78ms/step - loss: 4.8451 - accuracy: 0.1994 - val_loss: 3.4186 - val_accuracy: 0.4572\n",
      "Epoch 2/5\n",
      "664/664 [==============================] - ETA: 0s - loss: 2.0575 - accuracy: 0.7249\n",
      "Epoch 00002: val_loss improved from 3.41855 to 1.36455, saving model to Lenet10.hdf5\n",
      "664/664 [==============================] - 47s 71ms/step - loss: 2.0575 - accuracy: 0.7249 - val_loss: 1.3645 - val_accuracy: 0.8208\n",
      "Epoch 3/5\n",
      "664/664 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.9391\n",
      "Epoch 00003: val_loss improved from 1.36455 to 0.55694, saving model to Lenet10.hdf5\n",
      "664/664 [==============================] - 46s 69ms/step - loss: 0.6513 - accuracy: 0.9391 - val_loss: 0.5569 - val_accuracy: 0.9373\n",
      "Epoch 4/5\n",
      "664/664 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9858\n",
      "Epoch 00004: val_loss improved from 0.55694 to 0.30205, saving model to Lenet10.hdf5\n",
      "664/664 [==============================] - 43s 64ms/step - loss: 0.2288 - accuracy: 0.9858 - val_loss: 0.3020 - val_accuracy: 0.9665\n",
      "Epoch 5/5\n",
      "663/664 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9975\n",
      "Epoch 00005: val_loss improved from 0.30205 to 0.20671, saving model to Lenet10.hdf5\n",
      "664/664 [==============================] - 42s 63ms/step - loss: 0.0933 - accuracy: 0.9975 - val_loss: 0.2067 - val_accuracy: 0.9750\n",
      "Lenet11 20880\n",
      "Epoch 1/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 4.5402 - accuracy: 0.1729\n",
      "Epoch 00001: val_loss improved from inf to 3.09343, saving model to Lenet11.hdf5\n",
      "588/588 [==============================] - 37s 63ms/step - loss: 4.5398 - accuracy: 0.1729 - val_loss: 3.0934 - val_accuracy: 0.4679\n",
      "Epoch 2/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 2.0266 - accuracy: 0.6913\n",
      "Epoch 00002: val_loss improved from 3.09343 to 1.34736, saving model to Lenet11.hdf5\n",
      "588/588 [==============================] - 39s 66ms/step - loss: 2.0263 - accuracy: 0.6914 - val_loss: 1.3474 - val_accuracy: 0.8084\n",
      "Epoch 3/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.7790 - accuracy: 0.9156\n",
      "Epoch 00003: val_loss improved from 1.34736 to 0.62594, saving model to Lenet11.hdf5\n",
      "588/588 [==============================] - 43s 73ms/step - loss: 0.7789 - accuracy: 0.9156 - val_loss: 0.6259 - val_accuracy: 0.9215\n",
      "Epoch 4/5\n",
      "588/588 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.9797\n",
      "Epoch 00004: val_loss improved from 0.62594 to 0.29596, saving model to Lenet11.hdf5\n",
      "588/588 [==============================] - 24s 40ms/step - loss: 0.3065 - accuracy: 0.9797 - val_loss: 0.2960 - val_accuracy: 0.9708\n",
      "Epoch 5/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9949\n",
      "Epoch 00005: val_loss improved from 0.29596 to 0.17257, saving model to Lenet11.hdf5\n",
      "588/588 [==============================] - 24s 40ms/step - loss: 0.1348 - accuracy: 0.9949 - val_loss: 0.1726 - val_accuracy: 0.9804\n",
      "Lenet12 25920\n",
      "Epoch 1/5\n",
      "729/729 [==============================] - ETA: 0s - loss: 4.8559 - accuracy: 0.1693\n",
      "Epoch 00001: val_loss improved from inf to 3.36517, saving model to Lenet12.hdf5\n",
      "729/729 [==============================] - 30s 41ms/step - loss: 4.8559 - accuracy: 0.1693 - val_loss: 3.3652 - val_accuracy: 0.4313\n",
      "Epoch 2/5\n",
      "729/729 [==============================] - ETA: 0s - loss: 2.1309 - accuracy: 0.6869\n",
      "Epoch 00002: val_loss improved from 3.36517 to 1.39370, saving model to Lenet12.hdf5\n",
      "729/729 [==============================] - 30s 41ms/step - loss: 2.1309 - accuracy: 0.6869 - val_loss: 1.3937 - val_accuracy: 0.8137\n",
      "Epoch 3/5\n",
      "728/729 [============================>.] - ETA: 0s - loss: 0.7681 - accuracy: 0.9217\n",
      "Epoch 00003: val_loss improved from 1.39370 to 0.61988, saving model to Lenet12.hdf5\n",
      "729/729 [==============================] - 31s 43ms/step - loss: 0.7678 - accuracy: 0.9218 - val_loss: 0.6199 - val_accuracy: 0.9232\n",
      "Epoch 4/5\n",
      "728/729 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9798\n",
      "Epoch 00004: val_loss improved from 0.61988 to 0.31418, saving model to Lenet12.hdf5\n",
      "729/729 [==============================] - 33s 45ms/step - loss: 0.2955 - accuracy: 0.9799 - val_loss: 0.3142 - val_accuracy: 0.9645\n",
      "Epoch 5/5\n",
      "728/729 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9944\n",
      "Epoch 00005: val_loss improved from 0.31418 to 0.19450, saving model to Lenet12.hdf5\n",
      "729/729 [==============================] - 34s 47ms/step - loss: 0.1280 - accuracy: 0.9944 - val_loss: 0.1945 - val_accuracy: 0.9776\n",
      "Lenet13 24480\n",
      "Epoch 1/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.7820 - accuracy: 0.0030\n",
      "Epoch 00001: val_loss improved from inf to 5.76264, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 53s 76ms/step - loss: 5.7820 - accuracy: 0.0030 - val_loss: 5.7626 - val_accuracy: 0.0033\n",
      "Epoch 2/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.7632 - accuracy: 0.0028\n",
      "Epoch 00002: val_loss improved from 5.76264 to 5.75875, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 49s 71ms/step - loss: 5.7632 - accuracy: 0.0028 - val_loss: 5.7587 - val_accuracy: 0.0033\n",
      "Epoch 3/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.7606 - accuracy: 0.0031\n",
      "Epoch 00003: val_loss did not improve from 5.75875\n",
      "689/689 [==============================] - 47s 68ms/step - loss: 5.7606 - accuracy: 0.0031 - val_loss: 5.7644 - val_accuracy: 0.0033\n",
      "Epoch 4/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.7575 - accuracy: 0.0029\n",
      "Epoch 00004: val_loss did not improve from 5.75875\n",
      "689/689 [==============================] - 47s 69ms/step - loss: 5.7574 - accuracy: 0.0029 - val_loss: 5.7617 - val_accuracy: 0.0033\n",
      "Epoch 5/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.7572 - accuracy: 0.0032\n",
      "Epoch 00005: val_loss improved from 5.75875 to 5.75580, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 47s 68ms/step - loss: 5.7573 - accuracy: 0.0032 - val_loss: 5.7558 - val_accuracy: 0.0033\n",
      "Lenet13 24480\n",
      "Epoch 1/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 5.0626 - accuracy: 0.1452\n",
      "Epoch 00001: val_loss improved from 5.75580 to 3.55597, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 49s 71ms/step - loss: 5.0615 - accuracy: 0.1455 - val_loss: 3.5560 - val_accuracy: 0.3938\n",
      "Epoch 2/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 2.1421 - accuracy: 0.7005\n",
      "Epoch 00002: val_loss improved from 3.55597 to 1.26200, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 47s 69ms/step - loss: 2.1414 - accuracy: 0.7007 - val_loss: 1.2620 - val_accuracy: 0.8476\n",
      "Epoch 3/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 0.6678 - accuracy: 0.9408\n",
      "Epoch 00003: val_loss improved from 1.26200 to 0.50153, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 48s 70ms/step - loss: 0.6679 - accuracy: 0.9407 - val_loss: 0.5015 - val_accuracy: 0.9477\n",
      "Epoch 4/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9878\n",
      "Epoch 00004: val_loss improved from 0.50153 to 0.25016, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 48s 70ms/step - loss: 0.2366 - accuracy: 0.9878 - val_loss: 0.2502 - val_accuracy: 0.9751\n",
      "Epoch 5/5\n",
      "688/689 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9971\n",
      "Epoch 00005: val_loss improved from 0.25016 to 0.14519, saving model to Lenet13.hdf5\n",
      "689/689 [==============================] - 48s 69ms/step - loss: 0.0996 - accuracy: 0.9971 - val_loss: 0.1452 - val_accuracy: 0.9849\n",
      "Lenet14 22960\n",
      "Epoch 1/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 4.8828 - accuracy: 0.1248\n",
      "Epoch 00001: val_loss improved from inf to 3.37524, saving model to Lenet14.hdf5\n",
      "646/646 [==============================] - 43s 66ms/step - loss: 4.8828 - accuracy: 0.1248 - val_loss: 3.3752 - val_accuracy: 0.3876\n",
      "Epoch 2/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 2.2440 - accuracy: 0.6293\n",
      "Epoch 00002: val_loss improved from 3.37524 to 1.48793, saving model to Lenet14.hdf5\n",
      "646/646 [==============================] - 43s 67ms/step - loss: 2.2440 - accuracy: 0.6293 - val_loss: 1.4879 - val_accuracy: 0.7805\n",
      "Epoch 3/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.8937\n",
      "Epoch 00003: val_loss improved from 1.48793 to 0.67299, saving model to Lenet14.hdf5\n",
      "646/646 [==============================] - 44s 68ms/step - loss: 0.8971 - accuracy: 0.8937 - val_loss: 0.6730 - val_accuracy: 0.9155\n",
      "Epoch 4/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.9682\n",
      "Epoch 00004: val_loss improved from 0.67299 to 0.34763, saving model to Lenet14.hdf5\n",
      "646/646 [==============================] - 45s 70ms/step - loss: 0.3785 - accuracy: 0.9682 - val_loss: 0.3476 - val_accuracy: 0.9643\n",
      "Epoch 5/5\n",
      "645/646 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9930\n",
      "Epoch 00005: val_loss improved from 0.34763 to 0.18789, saving model to Lenet14.hdf5\n",
      "646/646 [==============================] - 45s 69ms/step - loss: 0.1686 - accuracy: 0.9930 - val_loss: 0.1879 - val_accuracy: 0.9830\n",
      "Lenet15 23040\n",
      "Epoch 1/5\n",
      "648/648 [==============================] - ETA: 0s - loss: 5.1958 - accuracy: 0.1098\n",
      "Epoch 00001: val_loss improved from inf to 4.14138, saving model to Lenet15.hdf5\n",
      "648/648 [==============================] - 43s 67ms/step - loss: 5.1958 - accuracy: 0.1098 - val_loss: 4.1414 - val_accuracy: 0.3095\n",
      "Epoch 2/5\n",
      "648/648 [==============================] - ETA: 0s - loss: 2.7340 - accuracy: 0.5880\n",
      "Epoch 00002: val_loss improved from 4.14138 to 1.76529, saving model to Lenet15.hdf5\n",
      "648/648 [==============================] - 44s 68ms/step - loss: 2.7340 - accuracy: 0.5880 - val_loss: 1.7653 - val_accuracy: 0.7448\n",
      "Epoch 3/5\n",
      "648/648 [==============================] - ETA: 0s - loss: 0.9988 - accuracy: 0.8894\n",
      "Epoch 00003: val_loss improved from 1.76529 to 0.74007, saving model to Lenet15.hdf5\n",
      "648/648 [==============================] - 45s 69ms/step - loss: 0.9988 - accuracy: 0.8894 - val_loss: 0.7401 - val_accuracy: 0.9206\n",
      "Epoch 4/5\n",
      "648/648 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.9724\n",
      "Epoch 00004: val_loss improved from 0.74007 to 0.36004, saving model to Lenet15.hdf5\n",
      "648/648 [==============================] - 45s 70ms/step - loss: 0.3679 - accuracy: 0.9724 - val_loss: 0.3600 - val_accuracy: 0.9653\n",
      "Epoch 5/5\n",
      "648/648 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9932\n",
      "Epoch 00005: val_loss improved from 0.36004 to 0.19724, saving model to Lenet15.hdf5\n",
      "648/648 [==============================] - 45s 69ms/step - loss: 0.1515 - accuracy: 0.9932 - val_loss: 0.1972 - val_accuracy: 0.9770\n",
      "Lenet16 20400\n",
      "Epoch 1/5\n",
      "574/574 [==============================] - ETA: 0s - loss: 4.8480 - accuracy: 0.1824\n",
      "Epoch 00001: val_loss improved from inf to 3.77371, saving model to Lenet16.hdf5\n",
      "574/574 [==============================] - 38s 66ms/step - loss: 4.8480 - accuracy: 0.1824 - val_loss: 3.7737 - val_accuracy: 0.3931\n",
      "Epoch 2/5\n",
      "573/574 [============================>.] - ETA: 0s - loss: 2.3568 - accuracy: 0.6759\n",
      "Epoch 00002: val_loss improved from 3.77371 to 1.64208, saving model to Lenet16.hdf5\n",
      "574/574 [==============================] - 39s 68ms/step - loss: 2.3560 - accuracy: 0.6760 - val_loss: 1.6421 - val_accuracy: 0.7775\n",
      "Epoch 3/5\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.9163\n",
      "Epoch 00003: val_loss improved from 1.64208 to 0.69693, saving model to Lenet16.hdf5\n",
      "574/574 [==============================] - 39s 69ms/step - loss: 0.8035 - accuracy: 0.9163 - val_loss: 0.6969 - val_accuracy: 0.9103\n",
      "Epoch 4/5\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9773\n",
      "Epoch 00004: val_loss improved from 0.69693 to 0.34439, saving model to Lenet16.hdf5\n",
      "574/574 [==============================] - 40s 70ms/step - loss: 0.2862 - accuracy: 0.9773 - val_loss: 0.3444 - val_accuracy: 0.9647\n",
      "Epoch 5/5\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9941\n",
      "Epoch 00005: val_loss improved from 0.34439 to 0.20726, saving model to Lenet16.hdf5\n",
      "574/574 [==============================] - 39s 68ms/step - loss: 0.1186 - accuracy: 0.9941 - val_loss: 0.2073 - val_accuracy: 0.9799\n",
      "Lenet17 20560\n",
      "Epoch 1/5\n",
      "578/579 [============================>.] - ETA: 0s - loss: 4.7700 - accuracy: 0.1869\n",
      "Epoch 00001: val_loss improved from inf to 3.52930, saving model to Lenet17.hdf5\n",
      "579/579 [==============================] - 37s 65ms/step - loss: 4.7694 - accuracy: 0.1869 - val_loss: 3.5293 - val_accuracy: 0.4144\n",
      "Epoch 2/5\n",
      "578/579 [============================>.] - ETA: 0s - loss: 2.1967 - accuracy: 0.6905\n",
      "Epoch 00002: val_loss improved from 3.52930 to 1.48309, saving model to Lenet17.hdf5\n",
      "579/579 [==============================] - 38s 65ms/step - loss: 2.1963 - accuracy: 0.6906 - val_loss: 1.4831 - val_accuracy: 0.8108\n",
      "Epoch 3/5\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.7655 - accuracy: 0.9205\n",
      "Epoch 00003: val_loss improved from 1.48309 to 0.65866, saving model to Lenet17.hdf5\n",
      "579/579 [==============================] - 38s 66ms/step - loss: 0.7654 - accuracy: 0.9204 - val_loss: 0.6587 - val_accuracy: 0.9207\n",
      "Epoch 4/5\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.9803\n",
      "Epoch 00004: val_loss improved from 0.65866 to 0.36180, saving model to Lenet17.hdf5\n",
      "579/579 [==============================] - 39s 68ms/step - loss: 0.2785 - accuracy: 0.9803 - val_loss: 0.3618 - val_accuracy: 0.9591\n",
      "Epoch 5/5\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9957\n",
      "Epoch 00005: val_loss improved from 0.36180 to 0.24304, saving model to Lenet17.hdf5\n",
      "579/579 [==============================] - 39s 68ms/step - loss: 0.1143 - accuracy: 0.9957 - val_loss: 0.2430 - val_accuracy: 0.9728\n",
      "Lenet18 20880\n",
      "Epoch 1/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 4.5126 - accuracy: 0.1932\n",
      "Epoch 00001: val_loss improved from inf to 3.03554, saving model to Lenet18.hdf5\n",
      "588/588 [==============================] - 37s 63ms/step - loss: 4.5118 - accuracy: 0.1933 - val_loss: 3.0355 - val_accuracy: 0.4569\n",
      "Epoch 2/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 1.9267 - accuracy: 0.7106\n",
      "Epoch 00002: val_loss improved from 3.03554 to 1.27811, saving model to Lenet18.hdf5\n",
      "588/588 [==============================] - 37s 63ms/step - loss: 1.9263 - accuracy: 0.7106 - val_loss: 1.2781 - val_accuracy: 0.8333\n",
      "Epoch 3/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.7131 - accuracy: 0.9286\n",
      "Epoch 00003: val_loss improved from 1.27811 to 0.56076, saving model to Lenet18.hdf5\n",
      "588/588 [==============================] - 37s 64ms/step - loss: 0.7129 - accuracy: 0.9286 - val_loss: 0.5608 - val_accuracy: 0.9416\n",
      "Epoch 4/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.9838\n",
      "Epoch 00004: val_loss improved from 0.56076 to 0.26303, saving model to Lenet18.hdf5\n",
      "588/588 [==============================] - 38s 65ms/step - loss: 0.2777 - accuracy: 0.9838 - val_loss: 0.2630 - val_accuracy: 0.9756\n",
      "Epoch 5/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9954\n",
      "Epoch 00005: val_loss improved from 0.26303 to 0.14952, saving model to Lenet18.hdf5\n",
      "588/588 [==============================] - 38s 64ms/step - loss: 0.1233 - accuracy: 0.9954 - val_loss: 0.1495 - val_accuracy: 0.9866\n",
      "Lenet19 22560\n",
      "Epoch 1/5\n",
      "634/635 [============================>.] - ETA: 0s - loss: 4.8312 - accuracy: 0.1559\n",
      "Epoch 00001: val_loss improved from inf to 3.53053, saving model to Lenet19.hdf5\n",
      "635/635 [==============================] - 40s 64ms/step - loss: 4.8302 - accuracy: 0.1561 - val_loss: 3.5305 - val_accuracy: 0.3963\n",
      "Epoch 2/5\n",
      "634/635 [============================>.] - ETA: 0s - loss: 2.2793 - accuracy: 0.6581\n",
      "Epoch 00002: val_loss improved from 3.53053 to 1.54361, saving model to Lenet19.hdf5\n",
      "635/635 [==============================] - 41s 64ms/step - loss: 2.2784 - accuracy: 0.6583 - val_loss: 1.5436 - val_accuracy: 0.7673\n",
      "Epoch 3/5\n",
      "634/635 [============================>.] - ETA: 0s - loss: 0.8644 - accuracy: 0.9027\n",
      "Epoch 00003: val_loss improved from 1.54361 to 0.69307, saving model to Lenet19.hdf5\n",
      "635/635 [==============================] - 42s 65ms/step - loss: 0.8642 - accuracy: 0.9027 - val_loss: 0.6931 - val_accuracy: 0.9100\n",
      "Epoch 4/5\n",
      "634/635 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.9743\n",
      "Epoch 00004: val_loss improved from 0.69307 to 0.35990, saving model to Lenet19.hdf5\n",
      "635/635 [==============================] - 42s 66ms/step - loss: 0.3388 - accuracy: 0.9743 - val_loss: 0.3599 - val_accuracy: 0.9566\n",
      "Epoch 5/5\n",
      "634/635 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9930\n",
      "Epoch 00005: val_loss improved from 0.35990 to 0.20782, saving model to Lenet19.hdf5\n",
      "635/635 [==============================] - 41s 64ms/step - loss: 0.1466 - accuracy: 0.9930 - val_loss: 0.2078 - val_accuracy: 0.9774\n",
      "Lenet20 20160\n",
      "Epoch 1/5\n",
      "567/567 [==============================] - ETA: 0s - loss: 4.8273 - accuracy: 0.1927\n",
      "Epoch 00001: val_loss improved from inf to 3.77308, saving model to Lenet20.hdf5\n",
      "567/567 [==============================] - 36s 63ms/step - loss: 4.8273 - accuracy: 0.1927 - val_loss: 3.7731 - val_accuracy: 0.4187\n",
      "Epoch 2/5\n",
      "567/567 [==============================] - ETA: 0s - loss: 2.4095 - accuracy: 0.6683\n",
      "Epoch 00002: val_loss improved from 3.77308 to 1.64582, saving model to Lenet20.hdf5\n",
      "567/567 [==============================] - 36s 64ms/step - loss: 2.4095 - accuracy: 0.6683 - val_loss: 1.6458 - val_accuracy: 0.7713\n",
      "Epoch 3/5\n",
      "567/567 [==============================] - ETA: 0s - loss: 0.8329 - accuracy: 0.9105\n",
      "Epoch 00003: val_loss improved from 1.64582 to 0.69643, saving model to Lenet20.hdf5\n",
      "567/567 [==============================] - 37s 65ms/step - loss: 0.8329 - accuracy: 0.9105 - val_loss: 0.6964 - val_accuracy: 0.9206\n",
      "Epoch 4/5\n",
      "567/567 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.9769\n",
      "Epoch 00004: val_loss improved from 0.69643 to 0.35651, saving model to Lenet20.hdf5\n",
      "567/567 [==============================] - 37s 66ms/step - loss: 0.2982 - accuracy: 0.9769 - val_loss: 0.3565 - val_accuracy: 0.9633\n",
      "Epoch 5/5\n",
      "567/567 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9943\n",
      "Epoch 00005: val_loss improved from 0.35651 to 0.22407, saving model to Lenet20.hdf5\n",
      "567/567 [==============================] - 37s 66ms/step - loss: 0.1213 - accuracy: 0.9943 - val_loss: 0.2241 - val_accuracy: 0.9767\n",
      "Lenet21 20880\n",
      "Epoch 1/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 4.6792 - accuracy: 0.1454\n",
      "Epoch 00001: val_loss improved from inf to 3.32300, saving model to Lenet21.hdf5\n",
      "588/588 [==============================] - 37s 64ms/step - loss: 4.6789 - accuracy: 0.1455 - val_loss: 3.3230 - val_accuracy: 0.3956\n",
      "Epoch 2/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 2.2000 - accuracy: 0.6359\n",
      "Epoch 00002: val_loss improved from 3.32300 to 1.53410, saving model to Lenet21.hdf5\n",
      "588/588 [==============================] - 38s 64ms/step - loss: 2.1999 - accuracy: 0.6359 - val_loss: 1.5341 - val_accuracy: 0.7658\n",
      "Epoch 3/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.9253 - accuracy: 0.8838\n",
      "Epoch 00003: val_loss improved from 1.53410 to 0.74879, saving model to Lenet21.hdf5\n",
      "588/588 [==============================] - 38s 65ms/step - loss: 0.9252 - accuracy: 0.8838 - val_loss: 0.7488 - val_accuracy: 0.8966\n",
      "Epoch 4/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.9650\n",
      "Epoch 00004: val_loss improved from 0.74879 to 0.41124, saving model to Lenet21.hdf5\n",
      "588/588 [==============================] - 39s 67ms/step - loss: 0.4032 - accuracy: 0.9650 - val_loss: 0.4112 - val_accuracy: 0.9559\n",
      "Epoch 5/5\n",
      "587/588 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9902\n",
      "Epoch 00005: val_loss improved from 0.41124 to 0.22760, saving model to Lenet21.hdf5\n",
      "588/588 [==============================] - 38s 65ms/step - loss: 0.1845 - accuracy: 0.9902 - val_loss: 0.2276 - val_accuracy: 0.9761\n",
      "Lenet22 21760\n",
      "Epoch 1/5\n",
      "611/612 [============================>.] - ETA: 0s - loss: 5.6653 - accuracy: 0.0037\n",
      "Epoch 00001: val_loss improved from inf to 5.65001, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 26s 42ms/step - loss: 5.6653 - accuracy: 0.0038 - val_loss: 5.6500 - val_accuracy: 0.0037\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 5.6471 - accuracy: 0.0039\n",
      "Epoch 00002: val_loss improved from 5.65001 to 5.63817, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 26s 42ms/step - loss: 5.6471 - accuracy: 0.0039 - val_loss: 5.6382 - val_accuracy: 0.0037\n",
      "Epoch 3/5\n",
      "611/612 [============================>.] - ETA: 0s - loss: 5.6442 - accuracy: 0.0035\n",
      "Epoch 00003: val_loss did not improve from 5.63817\n",
      "612/612 [==============================] - 27s 44ms/step - loss: 5.6443 - accuracy: 0.0035 - val_loss: 5.6454 - val_accuracy: 0.0037\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 5.6418 - accuracy: 0.0038\n",
      "Epoch 00004: val_loss did not improve from 5.63817\n",
      "612/612 [==============================] - 28s 46ms/step - loss: 5.6418 - accuracy: 0.0038 - val_loss: 5.6391 - val_accuracy: 0.0037\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 5.6401 - accuracy: 0.0039\n",
      "Epoch 00005: val_loss did not improve from 5.63817\n",
      "612/612 [==============================] - 29s 47ms/step - loss: 5.6401 - accuracy: 0.0039 - val_loss: 5.6406 - val_accuracy: 0.0037\n",
      "Lenet22 21760\n",
      "Epoch 1/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 4.4521 - accuracy: 0.2036\n",
      "Epoch 00001: val_loss improved from 5.63817 to 2.78462, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 30s 49ms/step - loss: 4.4521 - accuracy: 0.2036 - val_loss: 2.7846 - val_accuracy: 0.5087\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 1.8151 - accuracy: 0.7219\n",
      "Epoch 00002: val_loss improved from 2.78462 to 1.15256, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 29s 48ms/step - loss: 1.8151 - accuracy: 0.7219 - val_loss: 1.1526 - val_accuracy: 0.8474\n",
      "Epoch 3/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.9264\n",
      "Epoch 00003: val_loss improved from 1.15256 to 0.49392, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 29s 47ms/step - loss: 0.7022 - accuracy: 0.9264 - val_loss: 0.4939 - val_accuracy: 0.9513\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9811\n",
      "Epoch 00004: val_loss improved from 0.49392 to 0.24346, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 29s 47ms/step - loss: 0.2914 - accuracy: 0.9811 - val_loss: 0.2435 - val_accuracy: 0.9835\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9961\n",
      "Epoch 00005: val_loss improved from 0.24346 to 0.13225, saving model to Lenet22.hdf5\n",
      "612/612 [==============================] - 29s 47ms/step - loss: 0.1315 - accuracy: 0.9961 - val_loss: 0.1322 - val_accuracy: 0.9931\n",
      "Lenet23 25840\n",
      "Epoch 1/5\n",
      "727/727 [==============================] - ETA: 0s - loss: 5.2836 - accuracy: 0.0938\n",
      "Epoch 00001: val_loss improved from inf to 3.97459, saving model to Lenet23.hdf5\n",
      "727/727 [==============================] - 54s 74ms/step - loss: 5.2836 - accuracy: 0.0938 - val_loss: 3.9746 - val_accuracy: 0.2968\n",
      "Epoch 2/5\n",
      "727/727 [==============================] - ETA: 0s - loss: 2.6075 - accuracy: 0.5864\n",
      "Epoch 00002: val_loss improved from 3.97459 to 1.69054, saving model to Lenet23.hdf5\n",
      "727/727 [==============================] - 50s 69ms/step - loss: 2.6075 - accuracy: 0.5864 - val_loss: 1.6905 - val_accuracy: 0.7450\n",
      "Epoch 3/5\n",
      "726/727 [============================>.] - ETA: 0s - loss: 0.9499 - accuracy: 0.8895\n",
      "Epoch 00003: val_loss improved from 1.69054 to 0.74413, saving model to Lenet23.hdf5\n",
      "727/727 [==============================] - 48s 66ms/step - loss: 0.9493 - accuracy: 0.8897 - val_loss: 0.7441 - val_accuracy: 0.9025\n",
      "Epoch 4/5\n",
      "726/727 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.9731\n",
      "Epoch 00004: val_loss improved from 0.74413 to 0.39050, saving model to Lenet23.hdf5\n",
      "727/727 [==============================] - 48s 66ms/step - loss: 0.3559 - accuracy: 0.9731 - val_loss: 0.3905 - val_accuracy: 0.9520\n",
      "Epoch 5/5\n",
      "726/727 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9940\n",
      "Epoch 00005: val_loss improved from 0.39050 to 0.23470, saving model to Lenet23.hdf5\n",
      "727/727 [==============================] - 48s 66ms/step - loss: 0.1490 - accuracy: 0.9940 - val_loss: 0.2347 - val_accuracy: 0.9671\n",
      "Lenet24 23440\n",
      "Epoch 1/5\n",
      "659/660 [============================>.] - ETA: 0s - loss: 4.9064 - accuracy: 0.1620\n",
      "Epoch 00001: val_loss improved from inf to 3.60398, saving model to Lenet24.hdf5\n",
      "660/660 [==============================] - 43s 66ms/step - loss: 4.9060 - accuracy: 0.1620 - val_loss: 3.6040 - val_accuracy: 0.3887\n",
      "Epoch 2/5\n",
      "659/660 [============================>.] - ETA: 0s - loss: 2.2433 - accuracy: 0.6786\n",
      "Epoch 00002: val_loss improved from 3.60398 to 1.48074, saving model to Lenet24.hdf5\n",
      "660/660 [==============================] - 44s 66ms/step - loss: 2.2428 - accuracy: 0.6787 - val_loss: 1.4807 - val_accuracy: 0.7944\n",
      "Epoch 3/5\n",
      "659/660 [============================>.] - ETA: 0s - loss: 0.7759 - accuracy: 0.9191\n",
      "Epoch 00003: val_loss improved from 1.48074 to 0.65091, saving model to Lenet24.hdf5\n",
      "660/660 [==============================] - 44s 67ms/step - loss: 0.7757 - accuracy: 0.9191 - val_loss: 0.6509 - val_accuracy: 0.9266\n",
      "Epoch 4/5\n",
      "659/660 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9803\n",
      "Epoch 00004: val_loss improved from 0.65091 to 0.32727, saving model to Lenet24.hdf5\n",
      "660/660 [==============================] - 44s 67ms/step - loss: 0.2890 - accuracy: 0.9803 - val_loss: 0.3273 - val_accuracy: 0.9637\n",
      "Epoch 5/5\n",
      "659/660 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9955\n",
      "Epoch 00005: val_loss improved from 0.32727 to 0.20179, saving model to Lenet24.hdf5\n",
      "660/660 [==============================] - 44s 67ms/step - loss: 0.1208 - accuracy: 0.9955 - val_loss: 0.2018 - val_accuracy: 0.9744\n",
      "Lenet25 23120\n",
      "Epoch 1/5\n",
      "650/651 [============================>.] - ETA: 0s - loss: 4.8260 - accuracy: 0.1880\n",
      "Epoch 00001: val_loss improved from inf to 3.49502, saving model to Lenet25.hdf5\n",
      "651/651 [==============================] - 44s 68ms/step - loss: 4.8255 - accuracy: 0.1881 - val_loss: 3.4950 - val_accuracy: 0.4403\n",
      "Epoch 2/5\n",
      "650/651 [============================>.] - ETA: 0s - loss: 2.1637 - accuracy: 0.7031\n",
      "Epoch 00002: val_loss improved from 3.49502 to 1.48322, saving model to Lenet25.hdf5\n",
      "651/651 [==============================] - 45s 68ms/step - loss: 2.1637 - accuracy: 0.7030 - val_loss: 1.4832 - val_accuracy: 0.7946\n",
      "Epoch 3/5\n",
      "650/651 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.9257\n",
      "Epoch 00003: val_loss improved from 1.48322 to 0.61086, saving model to Lenet25.hdf5\n",
      "651/651 [==============================] - 44s 68ms/step - loss: 0.7407 - accuracy: 0.9257 - val_loss: 0.6109 - val_accuracy: 0.9291\n",
      "Epoch 4/5\n",
      "650/651 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.9799\n",
      "Epoch 00004: val_loss improved from 0.61086 to 0.28788, saving model to Lenet25.hdf5\n",
      "651/651 [==============================] - 45s 69ms/step - loss: 0.2758 - accuracy: 0.9799 - val_loss: 0.2879 - val_accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "650/651 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9952\n",
      "Epoch 00005: val_loss improved from 0.28788 to 0.17485, saving model to Lenet25.hdf5\n",
      "651/651 [==============================] - 45s 69ms/step - loss: 0.1161 - accuracy: 0.9952 - val_loss: 0.1749 - val_accuracy: 0.9810\n",
      "Lenet26 20960\n",
      "Epoch 1/5\n",
      "589/590 [============================>.] - ETA: 0s - loss: 4.9044 - accuracy: 0.1854\n",
      "Epoch 00001: val_loss improved from inf to 3.88581, saving model to Lenet26.hdf5\n",
      "590/590 [==============================] - 40s 67ms/step - loss: 4.9040 - accuracy: 0.1854 - val_loss: 3.8858 - val_accuracy: 0.3783\n",
      "Epoch 2/5\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4435 - accuracy: 0.6664\n",
      "Epoch 00002: val_loss improved from 3.88581 to 1.66506, saving model to Lenet26.hdf5\n",
      "590/590 [==============================] - 40s 68ms/step - loss: 2.4426 - accuracy: 0.6666 - val_loss: 1.6651 - val_accuracy: 0.7686\n",
      "Epoch 3/5\n",
      "589/590 [============================>.] - ETA: 0s - loss: 0.8272 - accuracy: 0.9118\n",
      "Epoch 00003: val_loss improved from 1.66506 to 0.70758, saving model to Lenet26.hdf5\n",
      "590/590 [==============================] - 39s 67ms/step - loss: 0.8269 - accuracy: 0.9118 - val_loss: 0.7076 - val_accuracy: 0.9127\n",
      "Epoch 4/5\n",
      "589/590 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9782\n",
      "Epoch 00004: val_loss improved from 0.70758 to 0.36824, saving model to Lenet26.hdf5\n",
      "590/590 [==============================] - 39s 67ms/step - loss: 0.2919 - accuracy: 0.9782 - val_loss: 0.3682 - val_accuracy: 0.9575\n",
      "Epoch 5/5\n",
      "589/590 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9952\n",
      "Epoch 00005: val_loss improved from 0.36824 to 0.23808, saving model to Lenet26.hdf5\n",
      "590/590 [==============================] - 39s 67ms/step - loss: 0.1179 - accuracy: 0.9952 - val_loss: 0.2381 - val_accuracy: 0.9699\n",
      "Lenet27 19920\n",
      "Epoch 1/5\n",
      "560/561 [============================>.] - ETA: 0s - loss: 4.8368 - accuracy: 0.1355\n",
      "Epoch 00001: val_loss improved from inf to 3.62256, saving model to Lenet27.hdf5\n",
      "561/561 [==============================] - 38s 67ms/step - loss: 4.8362 - accuracy: 0.1357 - val_loss: 3.6226 - val_accuracy: 0.3589\n",
      "Epoch 2/5\n",
      "560/561 [============================>.] - ETA: 0s - loss: 2.3940 - accuracy: 0.6315\n",
      "Epoch 00002: val_loss improved from 3.62256 to 1.62325, saving model to Lenet27.hdf5\n",
      "561/561 [==============================] - 38s 68ms/step - loss: 2.3935 - accuracy: 0.6315 - val_loss: 1.6232 - val_accuracy: 0.7701\n",
      "Epoch 3/5\n",
      "560/561 [============================>.] - ETA: 0s - loss: 0.9272 - accuracy: 0.8960\n",
      "Epoch 00003: val_loss improved from 1.62325 to 0.71945, saving model to Lenet27.hdf5\n",
      "561/561 [==============================] - 38s 67ms/step - loss: 0.9271 - accuracy: 0.8960 - val_loss: 0.7194 - val_accuracy: 0.9192\n",
      "Epoch 4/5\n",
      "560/561 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.9758\n",
      "Epoch 00004: val_loss improved from 0.71945 to 0.36498, saving model to Lenet27.hdf5\n",
      "561/561 [==============================] - 37s 65ms/step - loss: 0.3612 - accuracy: 0.9758 - val_loss: 0.3650 - val_accuracy: 0.9623\n",
      "Epoch 5/5\n",
      "560/561 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9940\n",
      "Epoch 00005: val_loss improved from 0.36498 to 0.22343, saving model to Lenet27.hdf5\n",
      "561/561 [==============================] - 37s 65ms/step - loss: 0.1528 - accuracy: 0.9940 - val_loss: 0.2234 - val_accuracy: 0.9739\n",
      "Lenet28 21120\n",
      "Epoch 1/5\n",
      "594/594 [==============================] - ETA: 0s - loss: 4.6599 - accuracy: 0.1775\n",
      "Epoch 00001: val_loss improved from inf to 3.19758, saving model to Lenet28.hdf5\n",
      "594/594 [==============================] - 39s 65ms/step - loss: 4.6599 - accuracy: 0.1775 - val_loss: 3.1976 - val_accuracy: 0.4640\n",
      "Epoch 2/5\n",
      "594/594 [==============================] - ETA: 0s - loss: 2.1058 - accuracy: 0.6827\n",
      "Epoch 00002: val_loss improved from 3.19758 to 1.40399, saving model to Lenet28.hdf5\n",
      "594/594 [==============================] - 38s 65ms/step - loss: 2.1058 - accuracy: 0.6827 - val_loss: 1.4040 - val_accuracy: 0.8120\n",
      "Epoch 3/5\n",
      "594/594 [==============================] - ETA: 0s - loss: 0.7899 - accuracy: 0.9163\n",
      "Epoch 00003: val_loss improved from 1.40399 to 0.61506, saving model to Lenet28.hdf5\n",
      "594/594 [==============================] - 38s 65ms/step - loss: 0.7899 - accuracy: 0.9163 - val_loss: 0.6151 - val_accuracy: 0.9280\n",
      "Epoch 4/5\n",
      "594/594 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.9784\n",
      "Epoch 00004: val_loss improved from 0.61506 to 0.31627, saving model to Lenet28.hdf5\n",
      "594/594 [==============================] - 38s 64ms/step - loss: 0.3106 - accuracy: 0.9784 - val_loss: 0.3163 - val_accuracy: 0.9711\n",
      "Epoch 5/5\n",
      "594/594 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9948\n",
      "Epoch 00005: val_loss improved from 0.31627 to 0.18405, saving model to Lenet28.hdf5\n",
      "594/594 [==============================] - 38s 65ms/step - loss: 0.1317 - accuracy: 0.9948 - val_loss: 0.1840 - val_accuracy: 0.9796\n",
      "Lenet29 20080\n",
      "Epoch 1/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 5.5804 - accuracy: 0.0042\n",
      "Epoch 00001: val_loss improved from inf to 5.54640, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 5.5804 - accuracy: 0.0042 - val_loss: 5.5464 - val_accuracy: 0.0030\n",
      "Epoch 2/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 5.2278 - accuracy: 0.0189\n",
      "Epoch 00002: val_loss improved from 5.54640 to 4.77946, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 5.2278 - accuracy: 0.0189 - val_loss: 4.7795 - val_accuracy: 0.0508\n",
      "Epoch 3/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 3.5934 - accuracy: 0.2844\n",
      "Epoch 00003: val_loss improved from 4.77946 to 2.55073, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 3.5919 - accuracy: 0.2847 - val_loss: 2.5507 - val_accuracy: 0.5105\n",
      "Epoch 4/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 1.7219 - accuracy: 0.7094\n",
      "Epoch 00004: val_loss improved from 2.55073 to 1.20927, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 1.7219 - accuracy: 0.7094 - val_loss: 1.2093 - val_accuracy: 0.8038\n",
      "Epoch 5/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 0.7489 - accuracy: 0.9123\n",
      "Epoch 00005: val_loss improved from 1.20927 to 0.57585, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 0.7487 - accuracy: 0.9124 - val_loss: 0.5759 - val_accuracy: 0.9258\n",
      "Lenet29 20080\n",
      "Epoch 1/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 4.7344 - accuracy: 0.1869\n",
      "Epoch 00001: val_loss did not improve from 0.57585\n",
      "565/565 [==============================] - 36s 64ms/step - loss: 4.7323 - accuracy: 0.1874 - val_loss: 3.3971 - val_accuracy: 0.4492\n",
      "Epoch 2/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 2.0427 - accuracy: 0.7166\n",
      "Epoch 00002: val_loss did not improve from 0.57585\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 2.0410 - accuracy: 0.7169 - val_loss: 1.3056 - val_accuracy: 0.8307\n",
      "Epoch 3/5\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.9365 ETA: 0s - loss: 0\n",
      "Epoch 00003: val_loss improved from 0.57585 to 0.56831, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 0.6744 - accuracy: 0.9365 - val_loss: 0.5683 - val_accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9850\n",
      "Epoch 00004: val_loss improved from 0.56831 to 0.27563, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 0.2479 - accuracy: 0.9851 - val_loss: 0.2756 - val_accuracy: 0.9701\n",
      "Epoch 5/5\n",
      "564/565 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9971\n",
      "Epoch 00005: val_loss improved from 0.27563 to 0.18370, saving model to Lenet29.hdf5\n",
      "565/565 [==============================] - 37s 65ms/step - loss: 0.1026 - accuracy: 0.9970 - val_loss: 0.1837 - val_accuracy: 0.9776\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #обучение второго уровня\n",
    "with codecs.open(\"Lenet.txt\", \"w\", encoding = 'utf-8') as file:\n",
    "    for clust in range(30):\n",
    "        X = np.array([Data_for_clustering.X_data[i] for i in [index for (index, item) in enumerate(list(kmeans)*80) if item == clust]])\n",
    "        Y = [Data_for_clustering.Y_data[i] for i in [index for (index, item) in enumerate(list(kmeans)*80) if item == clust]]\n",
    "        file.write(\"Lenet\"+str(clust) + \":\")\n",
    "        file.write(str(Y))\n",
    "        file.write(\"\\n\")\n",
    "        encoder = LabelEncoder().fit(Y)\n",
    "        Y = tf.keras.utils.to_categorical(encoder.transform(Y),  dtype='int8')\n",
    "        filepath_to_raw_model=\"Lenet\"+str(clust)+\".hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath=filepath_to_raw_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        callbacks_list = [checkpoint]\n",
    "        while True:\n",
    "            print(\"Lenet\"+str(clust) + \" \" + str(len(Y)))\n",
    "            LeModel = LeNet(X.shape[0]/80)\n",
    "            LeModel.compile(optimizer='adam',loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "            history = LeModel.fit(X, Y, epochs = 5, batch_size = 32,validation_split = 0.1, callbacks=callbacks_list,)\n",
    "            if(history.history['loss'][-1] < 0.5):\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "928eedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobile = load_model(\"content/MobileNet_current_30.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2a1a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = np.argmax(model_mobile.predict(Data_for_clustering.X_data[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4606a36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 28, 26, 16, 25], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "193a843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-54-e7400b865ba8>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-54-e7400b865ba8>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model_lenet = load_model(\"Lenet\"+str(cluster[i])+\".hdf5\")\n",
    "    with codecs.open(\"Lenet.txt\", \"r\", encoding = 'utf-8') as file:\n",
    "         for line in file:\n",
    "            if str(cluster[i]) in line:\n",
    "                encoder = LabelEncoder().fit(line[10:-3].split(\"', '\"))\n",
    "                break\n",
    "    print(encoder.inverse_transform(np.argmax(model_lenet.predict(Data_for_clustering.X_data[i:i+1])))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42ade99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet = load_model(\"Lenet\"+str(cluster[0])+\".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29ba9725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['垈']\n",
      "['佺']\n",
      "['鱹']\n",
      "['缵']\n",
      "['椝']\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)#тестирование\n",
    "for i in range(5):\n",
    "    model_lenet = load_model(\"Lenet\"+str(cluster[i])+\".hdf5\")\n",
    "    with codecs.open(\"Lenet.txt\", encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            if str(cluster[i]) in line:\n",
    "            #res = line[10:-3].split(\"', '\")\n",
    "                encoder = LabelEncoder().fit(line[10:-3].split(\"', '\"))\n",
    "                break\n",
    "    print(encoder.inverse_transform(np.argmax(model_lenet.predict(Data_for_clustering.X_data[i:i+1]), axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf871687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['垈', '佺', '鱹', '缵', '椝']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_for_clustering.Y_data[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
